#!/bin/bash

set -e
echo "Started run.sh"

{% set ngpu = nnodes*8 %}

source ../env.sh

[ -s WFN.h5 ] || \
  ln -snf $INPUT_LOCATION/{{problem}}/WFN_file/WFN_out.h5 ./WFN.h5
[ -s WFNq.h5 ] || \
  ln -snf $INPUT_LOCATION/{{problem}}/WFN_file/WFNq.h5 ./

for fname in WFN.h5 WFNq.h5; do
  if [ ! -s $fname ]; then
    echo "Unable to link to $fname"
    exit 1
  fi
done

cat >epsilon.inp <<.
epsilon_cutoff      4.0
number_bands       3000
begin qpoints
  0.0 0.0 0.05  1.0  1
end
# Comment this line if running out of memory
# os_opt_ffts 1
# Read from WFN.h5 and WFNq.h5. Only supported by Epsilon.
# dont_use_hdf5
use_wfn_hdf5

# We use pseudobands, so we can't check norms
dont_check_norms
comm_nonblocking_cyclic

mtxel_algo {{algo}}
chi_summation_algo {{algo}}
# not available until July 20, 2021
n_ffts_per_batch {{n_ffts_per_batch}}
.

cat >batch.sh <<.
#!/bin/bash
#SBATCH --nodes={{ nnodes }}
#SBATCH --job-name={{ problem }}
#SBATCH -G {{ ngpu }} 
#SBATCH -C gpu
#SBATCH -c 80
#SBATCH -t 30:00
#SBATCH -o stdout
#SBATCH -e stderr
#SBATCH -A m3502

source ../env.sh

echo "Started job on \$SLURM_JOB_NODELIST"

export OMP_PROC_BIND=spread
export OMP_PLACES=threads
{% if arch == "cc70" %}
export OMP_NUM_THREADS=5
srun -n {{ ngpu }} -c10 ../BerkeleyGW/bin/epsilon.cplx.x
{% else %}
export OMP_NUM_THREADS=8
#export PGI_ACC_TIME=1
srun -n {{ ngpu }} -c 16 --cpu-bind=core ../BerkeleyGW/bin/epsilon.cplx.x

srun -n {{ ngpu }} -c 16 --cpu-bind=core nsys profile -t nvtx,cuda --stats=true --force-overwrite true -s none -o epsilon_omp_%q{SLURM_PROCID} ../BerkeleyGW/bin/epsilon.cplx.x >profile.log 2>&1
{% endif %}

echo "COMPLETE"
.

sbatch batch.sh

